\documentclass[]{article}
\usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8

\usepackage{graphicx} %graphics files inclusion

\usepackage{dirtree} %directory tree visualisation
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{lscape}

\usepackage{url}
\usepackage{hyperref}
\hypersetup{breaklinks=true}
\urlstyle{same}
\usepackage{cite}

\usepackage[czech]{babel}
\floatname{algorithm}{Algoritmus}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\newcommand\grsize{0.95}

\raggedbottom
\clubpenalty 10000
\widowpenalty 10000

\oddsidemargin=-5mm
\evensidemargin=-5mm\marginparwidth=.08in \marginparsep=.01in
\marginparpush=5pt\topmargin=-15mm\headheight=12pt
\headsep=25pt
\footskip=30pt
\textheight=25cm
\textwidth=17cm
\columnsep=2mm
\columnseprule=1pt\parindent=15pt\parskip=2pt
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath} % Equations
\usepackage{amssymb} % Equations

\begin{document}
\begin{center}
\bf Semestralní projekt MI-PDP 2018/2019:\\[5mm]
    Paralelni algoritmus pro řešení problému pokrývání plochy dlaždicemi L\\[5mm] 
       Ladislav Martínek\\[2mm]
magisterske studijum, FIT CVUT, Thakurova 9, 160 00 Praha 6\\[2mm]
\today
\end{center}

\section{Definice problému}

Problém, který jsme měli v ráncí semestrálního projektu řešit se týkal pokrývání plochy dlaždicemi L. Tedy bylo nutné pokrýt obdelníkovou mřížku dlaždicemi tak, že se optimalizovala cena takového řešení daná funkcí, která je uveden níže. 


\subsection{Vstupní data}
$m$, $n$ = rozměry obdélníkové mřížky $R[1..m,1..n]$, skládající se z $m$ x $n$ políček.
Dlaždice dvou tvarů L3 a L4, viz obrázek níže \ref{obr1}.

  \begin{figure}[H]\centering
	\includegraphics[width=0.1\textwidth]{img/ll.png}
 	\label{obr1}
 \end{figure}


\noindent
$k <m*n $ = počet zakázaných políček v mřížce $R$.

\noindent
$D[1..k]$ = pole souřadnic $k$ zakázaných políček náhodně rozmístěných v mřížce $R$.

\noindent
Pro mřížku $R$ se zakázanými políčky danými polem $D$ najděte \textbf{pokrytí}  dlaždicemi L3 a L4 (viz obrázek \ref{obr1}) \textbf{s~maximální cenou}.
Dlaždice lze \textbf{pravoúhle otáčet} a \textbf{obracet} (viz obrázek \ref{obr2}).

    \begin{figure}[H]\centering
	\includegraphics[width=0.2\textwidth]{img/l2.png}
 	\label{obr2}
 \end{figure}
  
 \noindent 
Všechna možná otočení a obrácení dlaždice L3.

\noindent
\textbf{Cena} pokrytí (funkce, kterou v algoritmu optimalizuji), ve kterém zbylo $q$ nepokrytých nezakázaných políček, je zadána následovně: 

$$ 2 * \text{počet dlaždic tvaru L3} + 3 * \text{počet dlaždic tvaru L4} - 6*q. $$

\subsection{Výstup algoritmu}

Číslo udávající maximální cenu pokrytí a počty dlaždic L3 a L4 a počet nepokrytých políček a jejich souřadnice.

\noindent
Popis pokrytí mřížky $R$, např. $(m \cdot n)$ - maticí identifikátorů, kde každá dlaždice je jednoznačně určena políčky 
s unikátním ID $>=1$, nepokrytá políčka jsou prázdná a zakázaná jsou reprezentovaná znakem 'Z'. 
    
\begin{figure}[H]\centering
	\includegraphics[width=0.15\textwidth]{img/l-example.jpg}
 	\label{obr3}
 \end{figure}

    
    \noindent
Mřížka R o rozměrech $10$ x $5$ políček s $3$ zakázanými políčky pokrytá dlaždicemi.

\noindent
Řešení vždy existuje.
\textbf{Těsná dolní mez} na cenu pokrytí je $- 6 \cdot( n \cdot m-k)$, když není vložitelná žádná dlaždice L4/L3.
\textbf{Těsnou horní mez} na cenu pokrytí lze pro dané $m,n,k$ spočítat je možné vypočítat pomocí řešení diofantské rovnice
$4*a+5*b=n*m-k$, které vždy existuje (neboť 4 a 5 jsou nesoudělná čísla) a kde bereme kladné řešení s maximálním $b$. Tuto mez někdy není možné dosáhnout (závisí i na pozici zakázaných políček).


\subsection{Vstupní datové soubory}


Formát vstupního textového datového souboru je následující:

\noindent
1. řádka: celá čísla $m$ a $n$ - rozměry obdélníkové mřížky (počet řádků a sloupců)

\noindent
Další řádka: číslo $k$ - počet zakázaných políček v mřížce
následujících $k$ řádek obsahuje x,y souřadnice zakázaných políček.

\section{Sekvenční řešení}
Sekvenční algoritmus je typu prohledávání do hloubky s metodou větví a hranic (BB-DFS) s hloubkou prohle\-dávaní prostoru omezenou na $(m \cdot n-k)$.
Algoritmus končí, když je cena rovna horní mezi nebo když prohledá celý stavový prostor do hloubky $\frac{m \cdot n-k}{4}
$.

Po načtení dat ze souboru nebo případně z příkazové řádky (obojí lze specifikovat argumenty) jsou vytvořeny dvě instance třídy $sollution$ jedna pro aktuální zpracovávanou konfiguraci a druhá pro nejlepší dozaženou. Mřížku reprezentuju jako 2D vektor a zároveň si držím parametry jako počet prazdných polí za a před zpracovávaným poličkem, počet umístěných kostiček L3 a L4 a také velikost dané mřížky. Celý algoritmus pak využívá pouze dvě metody při řešení. 

První metodou je $nextFree$, která pro zadané souřednice nalezne nejbližší volné políčko (první volání je mimo hrací plochu na začátku). Druhá metoda je pro přídaní políčka na mapu $addOnMap$. Tato metoda bere souřadnici kam začínáme pokládat (pouze pokud lze jinak neproběhne úspěsně), dále relativní souřadnice ve vektoru dané kostičky, předchozí hodnotu na místě a novou hodnotu z counteru. Touto metodou lze také kostičky odebírat pří návratu v DFS při zadání původní hodnoty jako id kostičky, která na místě ležela. 

Z držených parametrů daneho rozložení je vždy spočítána cena a pokud se rovná maximální možné tak algoritmus končí. Dále pro optimalizi používám prořezávání, tedy pokud aktuální instance s teoretickým maximem pro zbývající volná pole nemůže překonat aktuální maximum tak větev také končí.

Níže jsou uvedeny časy sekvenčního řešení pro 3 vybrané instance.
 % jednoduchá tabulka

\begin{center}
\parbox{0cm}{\begin{tabbing}
 město XYZ ~~~~\= datum ~~~~
    \= 34 \kill
    \bfseries Instance \>
    \bfseries Čas (s) \>
    \\[2mm]
    pol13 \> 1091.261 \>  \\
    pol14 \> 640.393 \> \\
    pol15 \> 876.792 \> 
\end{tabbing}}
\end{center}

První instance je prázná pole o rozměrech 22x18. Druhá instance je pole 15x15 s zakazanými políčky v rozích přiblizně prostřed stěn a uprostřed. Poslední je 17x17 se vzorem 4 čverců rozmístěných přibližně symentricky, tak aby kolem sebe měli co nejvíc místa. Tedy celkem 16 zakázaných políček.
\section{Popis paralelního algoritmu a jeho implementace~v~OpenMP~-~taskový paralelismus}
Při implementaci v OpenMP a taskovém paralelismu jsem musel upravil sekvenční řešení. Implementaci pomocí taskového paralelismu ani nebudu dále využívat pro implementaci v MPI. Aby tento paralelismus šlo využít bylo nutné zařídit, aby v rekurzi byla předávána kopie řešení a ne pouze ukazatel na jednu kopii, to celé řešení zpomalovalo a pomocí tohoto řešení se mi nepodařílo dosáhnout obstojných výsledků. 

První použitá direktiva je omp parallel blok a v něm první volání rekurze s direktivou single, je nutné aby první volání provedlo jen jedno vlákno. Samotná rekurze je stejná jako u sekvenčního řešení s těmi rozdíly, že kontrola a update nejlepšího řešení probíhá v kritické sekci a při volaní rekurze a zanoření je použita direktiva omp task, která tvoří tasky. Hloubku zanoření kontroluju podle id kostiček položených na mapě. Tuto hlobku lze nastavit parametrem (-nN) na příkazové řádce stejně jako počet vláken (-nT). Přepínač -tp slouží pro výběr task paralelismu z možností datového (-dp) a nebo pouze rekurzivního na jednom vlákně (-r). Při zkoušení se neosvědčilo mít hloubku zanoření více ja 4 pro 16 jader.

\begin{verbatim}
./[BUILD BINARY] -f [PATH TO INSTANCE] -tp -nT [NUMBER OF THREADS] -nN [PLUNGE DEPTH]
\end{verbatim}


\section{Popis paralelního algoritmu a jeho implementace v OpenMP - datový paralelismus}\label{kap1}

Při implementaci datového paralelismu v OpenMP bylo nutné vytvořit instance problému, které budou distribuované mezi vlákna. Pro toto jsem využil algoritmus BFS, kterým v režimu jednoho vlakna generuje instance (předvyplněné mapy). Počet těchto instancí lze opět určit z příkazové řádky (-nN), ovšem v tomto případě je to minimální počet instancí (dáno větvícím faktorem maximálně o 16 víc), které se vygenerují. 

Pro samotnou implementaci pak využívám direktivu parralel for, který vytvoří daný počet vláken (-nT) a přiděluje jim přegenerované instance. Přidělování používám dynamické. V rekurzi pak pro kontrolu nejlepšího řešení používám direktivu pro kritickou sekci.

Tato implementace paralelního algorimu se mi zamlouvala líp a byla také mnohem efektivnější. To především z důvodu, že jednotlivé vlákna mohou prováděnou rekurzi jednoduše provádět na jedné instanci, což výpočet velice zrychluje. 

\begin{verbatim}
./[BUILD BINARY] -f [PATH TO INSTANCE] -tp -nT [NUMBER OF THREADS] -nN [NUMBER OF GENERATED]
\end{verbatim}


\section{Popis paralelniho algoritmu a jeho implementace v MPI}

Pro MPI bylo nutné přepracovat více části. V MPI je nutné kompilovat kód pomocí speciálního kompilátoru (mpic++) a spouštět pomocí mpirun, kde jsou vytvořeny jednotlivé procesy a je zajištěna komunikace mezi nimi. Implemetace algoritmu je Master-slave. Tedy jeden proces je jako hlavní a ostatní jako dělníci. (v mé pozdější implementaci jsou volná vlákna na masteru také pracující)

Slave proces má jednoduchou implementaci, hlavní částí je blokující příjem zprávy od mastera. V této zprávě bud přijme informaci o konci nebo práci, kterou má vykonávat. Pokud je konec, tak se ukončí. Pokud se jedná o práci, tak vykonává s instancí přesně to, co bylo popsáno v datovém paralelismu v kapitole \ref{kap1}. V kódu je jen jedno vychýlení a to, že po určitěm počtu kroků provádí neblokující broadcastování nejlepšího řešení, tento broadcast začíná vždy master pokud dostane nějaké nové lepší řešení. Toto bylo nezbytně nutné pro ořezavání problému, neboť se stávalo, že zůstával běžet jeden proces (s těžkou úlohou, která mohla být oříznuta), i když řešení bylo dávno nalezeno.

Implementace mastera ze začátku byla pouze na posílání a příjem zpráv. Master má frontu na slave procesy, který má poslat zprávu a neblokujícím send posílá vygenerované instance, která na začátku vygeneroval pomocí BFS stejně jako v datovém paralelismu a kontroluje zda zpráva opravdu odešla. Poté čeká na příchozí zprávu od slave procesu s řešením. Pokud je řešení lepší uloží ho a neblokujícím broadcastem broadcastuje novou nejlepší hodnotu. A Slave procesu pošle novou práci, pokud nějaká je. 

Zprávy v MPI posílám jako pole čísel, kde jsem do objektu řešení vytvořil serializaci právě do pole čísel.

Tuto implemetaci mastera jsem později ještě rozšíříl pomocí OpenMP, kde master umí využít i zbylá vlákna na procesoru k počítání. Používám k tomu zapnuté nested parallel, abych mohl vnořit vytváření vláken pro OpenMP. V masterovi jsou nejprve vytvořena dvě vlákna, kde jedno je klasický master a druhé je jakoby slave. K tomuto řešení jsem pro začátek ještě nastavil zámek, který je zamčený z důvodu, že by se vlákno spustilo dříve, než bude naplněná fronta. Zámek je uvolněn po naplnění fronty. 

Toto slave vlákno pak odebírá problémy z fronty a řeší je datovým paralelismem. Při práci s frontou a nejlepším řešením jsou proměné zamčené v kritické sekci, aby nedocházelo ke konfliktům s master vláknem (procesem). Pří této implementaci se vyskytli nějaké problémy především s kritickými sekcemi, které bylo nutné odladit. Výkonost se ukáže při testování tohoto řešení.

\begin{verbatim}
mpirun -n [NUMBER OF PROCESSES] [BUILD BINARY] -f [PATH TO INSTANCE] -mpi 
-nT [NUMBER OF THREADS] -nN [NUMBER OF GENERATED IN PROCESS] 
-nNP [NUMBER OF GENERATED IN MASTER]
\end{verbatim}

\section{Naměřené výsledky a vyhodnocení}
Zde uvedu naměřené výsledky ze spuštěných experimentů a výpočetním klastru star. Pokusil jsem se neprve zkoumat závislost na velikosti generovaných problémů a to jak na úrovni paralelismu na jednom procesoru, tak na úrovni komunikace mezi vypočetními jednotkami. Dále pak zrychlení s počtem použití výpočetních jader.

\subsection{Závislost výpočetního času na počtu generovaných tasků jeden výpočetní uzel}

Jak je vidět na grafech níže je tento problém velice datově závislý. Na grafu \ref{a:nn20t} je závislost počtu generovaných instancí na jednom CPU na výpočetní čas. V grafu není vidět žádná závislost a vypočetní čas se pohybyje ve velkém rozsahu. Tento test byl prováděn na jednom vypočetním uzlu na 20 jádrech. V tomto případě jsem zde očekával nějakou závislost, ale časy by nejspíš rostly až kdyby generovaní tasků bralo velkou část času, což se děje spíše až uz sta tisíců. V tomto případě tak počáteční vygenerování rozložení určovalo jak rychle dojde k prořezání.

Pro další testy se u jednotlivých instancí budu snažit "netrefit" peaky, ale tak aby algoritmus běžel co nejrychleji.


\begin{figure}[H]\centering
	\includegraphics[width=1\textwidth]{img/nN20T}
 	\caption{Závislost počtu vygenerovaných počátečních konfigurací pomocí BFS pro datový paralelismus na jednom procesoru}\label{a:nn20t}
 \end{figure}

\subsection{Závislost výpočetního času na počtu generovaných tásků pro MPI}
Na grafu \ref{a:nnP20t} je již závislost patrnější a generování rozložení pro jednotlivé procesy je dobré co nejvíc minimalizovat. Tedy snížit počet komunikací po síti a nechat počítat na jednotlivých procesorech. Test byl prváděn na procesorech o 20 jádrech. Výsledky by možná byly odlišné při velkém počtu uzlů s méně jádry. Pro další testy držím tedy počet generovaných tasků na nízkém čísle.

\begin{figure}[H]\centering
	\includegraphics[width=1\textwidth]{img/nNP60T}
 	\caption{Závislost počtu vygenerovaných počátečních konfigurací pomocí BFS pro datový paralelismus na jednom procesoru}\label{a:nnP20t}
 \end{figure}


\subsection{Závislost výpočetního času na počtu vypočetních jader na jednom výpočetním uzlu}

Dálé jsem měříl algoritmus na jednom procesoru a zkoušel zvyšovat počet výpočetních jader. Na grafu \ref{a:dpp} je vidět klesající výpočetní čas s počtem jader, ale opět je to velice datově závislé. Také je na grafu paralelní zrychlení vidět superlineární zrychlení, které nastavá právě díky prořezávání, které proběhne optimálněji a výpočet se ukončí rychle.
 
 	\begin{figure}[H]
	\centering
    \begin{minipage}[c]{0.49\textwidth}
        \centering\includegraphics[width=\textwidth]{img/nTdp} 
    \end{minipage}
    \begin{minipage}[c]{0.49\textwidth}
        \centering \includegraphics[width=\textwidth]{img/parralelEfectivitydp} 
    \end{minipage}
    \\
   \caption{Na levém je obrázek výpočetního času na počtu jader. Na právém je efektivita.}\label{a:dpp}
    \end{figure}
    
\begin{figure}[H]\centering
	\includegraphics[width=0.8\textwidth]{img/parralelFastdp}
 	\caption{Na obrázku je graf paralelního zrychlení}\label{a:pfdp}
 \end{figure}


\subsection{Závislost výpočetního času na počtu vypočetních jader v klusteru}

V klusteru jsem díky pracujícímu masterovi testoval konfiguraci i pro 2 počítače v klastru a i pro 3. Je jasně patrné, že při 3 vypočetních uzlech se podařilo dosáhnout lepších časů i díky tomu, že v klusteru bylo k dispozici o 20 více jader.

\begin{figure}[H]
	\centering
    \begin{minipage}[c]{0.49\textwidth}
        \centering\includegraphics[width=\textwidth]{img/nTmpi2} 
    \end{minipage}
    \begin{minipage}[c]{0.49\textwidth}
        \centering \includegraphics[width=\textwidth]{img/nTmpi3} 
    \end{minipage}
    \\
   \caption{Na levém je obrázek výpočetního času na počtu jader pro dva počítače v klusteru. Na právém je stejný graf pro 3 počítače v klusteru.}\label{a:mpii}
    \end{figure}
    
    
Dále jsem z naměřených dat udělal grafy pro paralelní zrychlení a efektivitu, kterou jsem udělal pouze pro 3 vypočetní uzly. Na grafu níže \ref{a:pfmpi2} je graf paralelního zrychlení pro dva uzly. Při porovnání s grafem \ref{a:pfdp} je tento graf o poznání horší. Toto zpomalení je způsobeno komunikací mezi uzly a nemožností práce nad jednou sdílenou pamětí. Později bude pak dobré porovnat tento graf pro tři výpočetní uzly, který je uveden níže, kde očekávám lepší výsledek.


\begin{figure}[H]\centering
	\includegraphics[width=0.68\textwidth]{img/parralelFastmpi2}
 	\caption{Na obrázku je graf paralelního zrychlení}\label{a:pfmpi2}
 \end{figure}
 
 Na grafu \ref{a:pfmpi3} je vidět paralelní zrychlení pro klastr se třemi výpočetnímy uzly a maximálním počtem 60 jader. Je vidět, že zrychlení pro instance 14 a 15 není takové, protože zde kvůli zakázaným políčkům není tak jednoduché najít optimální řešení nebo prořezávat, naopak pro prázdnou instanci 13 se prořezaní někdy podařilo což vedlo k super lineárnímu zrychlení.
 
 \begin{figure}[H]\centering
	\includegraphics[width=0.68\textwidth]{img/parralelFastmpi3}
 	\caption{Na obrázku je graf paralelního zrychlení}\label{a:pfmpi3}
 \end{figure}

Na posledním obrázku \ref{a:eempi3} je vidět efektivita v závislsti na výpočetních jádrech. Graf koresponduje s paralelním zrychlení a jsou zde vidět dva peaky efektivity výšší než 1 přeně, kde došlo k superlineánímu paralelnímu zrychlení.
 \begin{figure}[H]\centering
	\includegraphics[width=0.7\textwidth]{img/parralelEfectivitympi3}
 	\caption{Na obrázku je graf paralelního zrychlení}\label{a:eempi3}
 \end{figure}

\subsection{Analýza a hodnocení}

Paralelní algoritmus pro jeden procesor s datovým paralelismem funguje velice dobře. Z výsledků MPI programu je vidět, že by implementace mohla být určitě vylepšena ve více ohledech. První úzké místo je broadcastování zlepšujícího řešení, kde kontrola i četnost tohoto volání jde řídit. Dále zlepšit posílání jednotlivých úloh, kde by se nemusel posílat celý grid a neprovádět pokáždé serializaci, ale zefektivnit komunikaci. Dále také implementace na vláknech mastera, kterou jsem sice provedl, ale jednotlivá vlákna trávila příliž času v kritických sekcí, což bych v dalších pokusech omezil. 

Škálovatelnost algoritmu je možná v mezích, kdy ještě BFS pro generování tasků bude časově přijatelné k času řešení celého problému. Podle naměřených výsledků bych odhadoval, že lze problém dobře škálovat, protože lze velice rychle vygenerovat velký počet tasků na začátku a z měření se ukázalo, že pro počet uzlů není tak důležité vytvořit více tásků. Dále škálovatelnost také může výrazně ovlivnit komunikace přes master uzel, kdy by již nestačil odbavovat komunikaci. Toto by šlo vyřešit celkovou decentralizací algoritmu.

Celý problém byl s velkým větvením stromu a také velice datové závislý, kde rychlost velmi ovlivnilo jen počáteční generovaní rozložení. Dobrá trefa v tomto generování mohla umožnit témeř okamžité prořezání a superlineární zrychlení, protože se již zbytek stromu nemusel procházet, zatímco při řešení na jednom vlákně bylo řešení v procházení stromu deterministický a prořezání proběhlo vždy stejně.

\section{Závěr}

Semestrální práce byla velice zajímavá a práce na ní přínosná především z hlediska ošahání si knihoven pro psaní paralelních programů jinak než přes samotné vytváření vláken pomocí Thread. Tato implementace může být velice efektivní a to především v přehlednosti kódu a časové náročnosti psaní programu. Dále také vyzkoušení si programovaní pro vypočetní klastr, kde je nutné aby spolu jednotlivé procesy komunikovali jinak než přes sdílenou paměť. Semestrální práce nám dala možnost si vyzkoušet různé přístupy a možnost s knihovnami experimentovat.

\end{document}
